\chapter{Mobility Prediction}

Mobility prediction is the task of predicting mobility information of individuals or collections of individuals given their historical mobility data. The target of prediction may be the trajectory of an individual (either a full path or the next location), the destination of a trip, the happening of certain events (e.g., the probability of a car crash occurring), or a combination of the above. For collective targets, the prediction may produce aggregate flows described by an Origin-Destination matrix, or events that involve multiple individuals at once (e.g., a traffic jam).

Approaches are driven by two perspective: the first considers movement as continuous, expressing points as latitude-longitude pairs, and interprets prediction as reconstructing the precise movements of individuals; the second considers movement as discrete, where points are interpreted as visited areas (PoIs, mobile phone cells, tessellation cells), and prediction becomes predicting which cell ID or sequence of cell IDs describes the future movement of the individual.

\section{Prediction Tools}

\subsection{Hidden Markov Models}

A \textbf{Markov chain} is a standard way to model sequential stochastic processes. It is used to estimate the probability of sequences of events, called states, that take values from a finite set (e.g., words, weather conditions, stock prices). The assumption  is that the probability of an event happening depends only on the previous event (``Markov assumption''):
\begin{equation*}
    P(q_i | q_1 q_2 \dots q_{i-1}) = P(q_i | q_{i-1})
\end{equation*}
The model can be represented as a simple \textbf{transition matrix} $P_{ij}$, containing the probability of transitioning from each state $i$ to each state $j$. Formally, a Markov chain is defined by:
\begin{itemize}[noitemsep]
    \item A set of states $Q = \{q_1, q_2, \dots, q_n\}$;
    \item A vector of prior probabilities $\Pi = \{\pi_1, pi_2, \dots, \pi_n\}, \quad \pi_i = P(S_0 = q_i)$;
    \item A transition matrix $P = \{ a_{ij} \}, \quad i,j \in [1,n]$, such that $a_{ij} = P(q_j|q_i)$.
\end{itemize}
Learning $\Pi$ and $Q$ is straightforward, as they can be estimated from the data by counting the frequency of each state in $Q$.

A \textbf{Hidden Markov Model} (\textbf{HMM}) calculates the probability of sequences of events by separating them between observed and hidden. The observed states (the ones that actually constitute the sequence) are ``emitted'' by the hidden ones, meaning that their probability depends only on the current hidden state. Formally, a hidden Markov model is defined by:
\begin{itemize}[noitemsep]
    \item A set of states $Q = \{q_1, q_2, \dots, q_n\}$;
    \item A set of observables $O = \{o_1, o_2, \dots, o_m\}$
    \item A vector of prior probabilities $\Pi = \{\pi_1, pi_2, \dots, \pi_n\}, \quad \pi_i = P(S_0 = q_i)$;
    \item A transition matrix $P = \{ a_{ij} \}, \quad i,j \in [1,n]$, such that $a_{ij} = P(q_j|q_i)$;
    \item A matrix of emission probabilities $P_E = \{p_{s\rightarrow o}\}, s \in Q, o \in O$, such that $p_{s \rightarrow o} = p(o|s)$.
\end{itemize}
The input of the model is a sequence of observations drawn from a finite vocabulary. The model can be used to solve three problems: likelihood, decoding, and learning.

\paragraph{Likelihood} Given an HMM $\lambda=(A,B)$ and an observation sequence $O$, determine the likelihood $P(O|\lambda)$. The naive approach to calculating the likelihood of a sequence is to sum the probabilities of that sequence being generated by all possible hidden state sequences. Since the number of hidden and observable states tends to be large in real tasks, this approach is infeasible as it has exponential complexity ($O(|Q|^{|O|})$). Insteadm an efficient algorithm called \textbf{forward algorithm} is used. It is a kind of dynamic programming algorithm with a $O(|Q|^2 |O|)$ complexity.

\paragraph{Decoding} Given as input an HMM $\lambda = (A,B)$, and a sequence of observations $O=o_1, o_2, \dots, o_T$, find the most probable sequence of states  $Q = q_1, q_2, \dots, q_T$. The most common decoding algorithm is the \textbf{Viterbi algorithm}, which is also a dynamic programming algorithm.

\paragraph{Learning} Given an observation sequence $O$ and the set of possible states in the HMM, learn the HMM parameters $A$ and $B$. The standard algorithm for HMM training is \textbf{forward-backward algorithm} (or \textbf{Baum-Welch algorithm}), a special case of the Expectation-Maximization algorithm, which is an iterative method used to find the maximum likelihood estimates of parameters in statistical models. A trained HMM can be used to predict the next value in a sequence using:
\begin{itemize}
    \item \textbf{Point estimation}, by simply appending the state that, once appended to the sequence, maximizes the likelihood:
    \begin{align*}
        o_T &= \arg \max_o P(O_T = o | o_1, o_2, \dots, o_{T-1}) =\\
        &= \arg \max_o \frac{P(o_1, o_2, \dots, o_{T-1}, O_T = o)}{P(o_1, o_2, \dots, o_{T-1})} = \\
        &= \arg \max_o P(o_1, o_2, \dots, o_{T-1}, O_T = o)
    \end{align*}
    \item \textbf{Conditional expectation} (for numerical data only), which calculates the expected value of the next observation given the current sequence:
    \begin{equation*}
        \mathbb{E}[O_T|o_1, o_2, \dots, o_{T-1}] = \sum_o o \cdot P(O_T = o | o_1, o_2, \dots, o_{T-1}) 
    \end{equation*}
\end{itemize}
When applied to mobility data, usually the first step is a discretization phase, in which each trajectory is converted into a sequence of cell/area IDs (the observations), and the HMM is trained on these sequences. Alternatively, data is kept as is, making emissions 2-dimensional (latitude and longitude pairs), and assuming a Gaussian distribution for the emissions.

\subsection{Pattern-based Prediction}

Mobility patterns can be used for predictions other than simply highlighting frequent sequences of visits/events. For example, after finding some frequent sequences of visits, and observing a trajectory up until a certain point, we may estimate the destination of the individual's trip by looking at the pattern that best matches the trajectory so far, and taking the last location in it. Patterns can be either treated as Booleans (the pattern either does or does not occur in the trajectory), or as numerical values that represent how strongly a pattern occurs. Standard Machine Learning models can be then applied to automatically learn from the patterns and predict the desired target variable.

Patterns can be extracted using various criteria; the most common is frequency, where the goal is to find sequences that express significant features, but can also be extracted based on their discriminatory power, which may find infrequent patterns that are useful.

The typical pipeline for pattern-based prediction is:
\begin{enumerate}[noitemsep]
    \item The user history is collected, representing the sequence of spatio-temporal points the user visited within a certain time frame;
    \item The data is split into several trips, by identifying the stop points and the movements among them;
    \item The resulting individual trajectories are grouped using a clustering algorithm with an appropriate spatio-temporal distance function;
    \item Clusters with few elements are pruned as they do not represent significant patterns;
    \item The medoid of each cluster is calculated, and the resulting set of medoids becomes the user's routine movements.
\end{enumerate}
This is repeated for multiple users to form a pool of patterns. When a prediction is done for a specific user, the algorithm first tries to match the user's trajectory with its own patterns, and if none of them are good enough, those of the general population are analyzed.

\subsection{Neural Networks}

The three main architectures used are \textbf{recurrent neural networks} (RNNs), \textbf{convolutional neural networks} (CNNs), and \textbf{generative adversarial networks} (GANs).

RNNs are neural networks where each hidden unit has an additional self-connection that allows the net to maintain a state. The state is updated each time an instance goes through the net, and is connected to its corresponding unit with a weighted edge that is trained along with the other weights. The model is especially useful fr sequential data, as it uses past information to influence the prediction of the current instance (e.g., predicting a word in a sentence knowing which words came before it). Standard RNNs have issues with capturing long-term dependencies between input and output, suffering from the ``vanishing gradient'' problem, so an advanced variant of RNNs called \textbf{Long Short-Term Memory networks} (LSTMs) are used instead. LSTM units have a more sophisticated structure with gates that control the flow of information. A typical LSTM scheme is the Encoder-Decoder architecure, where training is done in two steps: the \textbf{encoder} step, during which the sequence data is transformed in a latent representation, and the \textbf{decoder} step, where the output of the encoder is processed to generate a sequence until a special token is generated to signal the end.

CNNs are used mostly for image processing, and can be used in conjuction wit LSTM or other sequence-based models: CNNs capture spatial relations and are able to identify objects and features, while LSTM captures temporal relation and movement.

GANs are a type of model where two neural networks are trained to ``compete'' with each other in a zero-sum game. The \textbf{generator} tries to produce synthetic data that is as similar as possible to the real data, while the \textbf{discriminator} tries to distinguish between real and fake data. Initially, the generator produces a set of synthetic instances by processing a random input (e.g., a latent representation of a trajectory), and the discriminator is trained on a dataset composed of those fake instances and real instances from an actual dataset. The two models are then trained in parallel, trying to make the generator capable of producing convincingly realisting data, and the discriminator capable of telling if an instance comes from the generator or not. Both generator and discriminator can be any suitable model, depending on the type of data to learn from.